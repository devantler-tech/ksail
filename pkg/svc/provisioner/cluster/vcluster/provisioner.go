package vclusterprovisioner

import (
	"context"
	"fmt"
	"os"
	"slices"
	"strings"

	"github.com/devantler-tech/ksail/v5/pkg/svc/provider"
	"github.com/devantler-tech/ksail/v5/pkg/svc/provisioner/cluster/clustererr"
	loftlog "github.com/loft-sh/log"
	"github.com/loft-sh/vcluster/pkg/cli"
	cliconfig "github.com/loft-sh/vcluster/pkg/cli/config"
	"github.com/loft-sh/vcluster/pkg/cli/flags"
	"github.com/sirupsen/logrus"
)

// defaultVClusterName is used when no name is provided.
const defaultVClusterName = "vcluster-default"

// defaultChartVersion is the vCluster Helm chart version used by the Docker driver.
// This MUST match a published ghcr.io/loft-sh/vcluster-pro tag that supports the
// experimental.docker config generated by the SDK (github.com/loft-sh/vcluster).
// Must be available on ghcr.io/loft-sh/vcluster-pro as an OCI artifact.
const defaultChartVersion = "0.32.0-alpha.2"

// defaultKubernetesVersion overrides the SDK's default Kubernetes version.
// The SDK defaults to v1.35.0, but ghcr.io/loft-sh/kubernetes:v1.35.0-full ships
// a corrupt kine binary (9 bytes "Not Found" instead of ELF) on both amd64 and
// arm64. This causes the vCluster service to crash-loop with "exec format error".
// v1.34.0 is the latest version with a working kine binary.
// Remove this override once the upstream v1.35.0 image is fixed.
const defaultKubernetesVersion = "v1.34.0"

// connectMaxAttempts is the number of times to retry ConnectDocker after cluster
// creation. The SDK's waitForVCluster has a hardcoded 3-minute readiness timeout
// which is too short for CI runners. Retrying gives an effective timeout of
// ~9 minutes (3 attempts x 3 minutes).
const connectMaxAttempts = 3

// Provisioner implements the cluster provisioner interface for vCluster's Docker
// driver (Vind). Create and Delete use the vCluster Go SDK directly, while
// Start/Stop/List/Exists delegate to the Docker infrastructure provider.
type Provisioner struct {
	name          string
	valuesPath    string
	infraProvider provider.Provider
}

// NewProvisioner constructs a new vCluster provisioner.
//
// Parameters:
//   - name: default cluster name (used when no name is passed to methods)
//   - valuesPath: optional path to a vcluster.yaml values file
//   - infraProvider: Docker infrastructure provider for Start/Stop/List/Exists
func NewProvisioner(
	name string,
	valuesPath string,
	infraProvider provider.Provider,
) *Provisioner {
	if name == "" {
		name = defaultVClusterName
	}

	return &Provisioner{
		name:          name,
		valuesPath:    valuesPath,
		infraProvider: infraProvider,
	}
}

// SetProvider sets the infrastructure provider for node operations.
// This implements the ProviderAware interface.
func (p *Provisioner) SetProvider(prov provider.Provider) {
	p.infraProvider = prov
}

// Create provisions a vCluster using the Docker driver via the vCluster Go SDK.
//
// Cluster creation and connect are split into two phases because the SDK's
// ConnectDocker has a hardcoded 3-minute readiness timeout (waitForVCluster)
// which is too short for CI runners. We retry ConnectDocker up to
// connectMaxAttempts times, giving an effective timeout of ~9 minutes.
func (p *Provisioner) Create(ctx context.Context, name string) error {
	target := p.resolveName(name)

	opts := &cli.CreateOptions{
		Driver:       "docker",
		ChartVersion: defaultChartVersion,
		Connect:      false,
		Upgrade:      false,
		Distro:       "k8s",
	}

	valuesFiles, cleanup, err := buildValuesFiles(p.valuesPath)
	if err != nil {
		return fmt.Errorf("failed to prepare values files: %w", err)
	}
	defer cleanup()

	opts.Values = valuesFiles

	globalFlags := newGlobalFlags()
	logger := newStreamLogger()

	err = cli.CreateDocker(ctx, opts, globalFlags, target, logger)
	if err != nil {
		return fmt.Errorf("failed to create vCluster: %w", err)
	}

	return connectWithRetry(ctx, globalFlags, target, logger)
}

// connectWithRetry calls ConnectDocker in a retry loop to work around the SDK's
// hardcoded 3-minute readiness timeout. Each attempt gets a fresh 3-minute
// window; on retry the kubeconfig is already available so only the readiness
// poll is retried.
func connectWithRetry(
	ctx context.Context,
	globalFlags *flags.GlobalFlags,
	clusterName string,
	logger loftlog.Logger,
) error {
	connectOpts := &cli.ConnectOptions{
		UpdateCurrent: true,
	}

	var lastErr error

	for attempt := range connectMaxAttempts {
		if attempt > 0 {
			logger.Infof(
				"Retrying vCluster connect (attempt %d/%d)...",
				attempt+1, connectMaxAttempts,
			)
		}

		lastErr = cli.ConnectDocker(
			ctx, connectOpts, globalFlags, clusterName, nil, logger,
		)
		if lastErr == nil {
			return nil
		}

		logger.Warnf(
			"vCluster connect attempt %d/%d failed: %v",
			attempt+1, connectMaxAttempts, lastErr,
		)
	}

	return fmt.Errorf(
		"failed to connect to vCluster after %d attempts: %w",
		connectMaxAttempts, lastErr,
	)
}

// Delete removes a vCluster using the Docker driver via the vCluster Go SDK.
// Returns clustererr.ErrClusterNotFound if the cluster does not exist.
func (p *Provisioner) Delete(ctx context.Context, name string) error {
	target := p.resolveName(name)

	exists, err := p.Exists(ctx, target)
	if err != nil {
		return fmt.Errorf("failed to check cluster existence: %w", err)
	}

	if !exists {
		return fmt.Errorf("%w: %s", clustererr.ErrClusterNotFound, target)
	}

	opts := &cli.DeleteOptions{
		Driver:         "docker",
		DeleteContext:  true,
		IgnoreNotFound: true,
	}

	globalFlags := newGlobalFlags()
	logger := newStreamLogger()

	// platformClient is nil for local Docker-based clusters (no platform integration).
	err = cli.DeleteDocker(ctx, nil, opts, globalFlags, target, logger)
	if err != nil {
		return fmt.Errorf("failed to delete vCluster: %w", err)
	}

	return nil
}

// Start starts a stopped vCluster by starting its Docker containers.
// Delegates to the infrastructure provider for container operations.
func (p *Provisioner) Start(ctx context.Context, name string) error {
	return p.withProvider(ctx, name, "start", func(ctx context.Context, clusterName string) error {
		return p.infraProvider.StartNodes(ctx, clusterName)
	})
}

// Stop stops a running vCluster by stopping its Docker containers.
// Delegates to the infrastructure provider for container operations.
func (p *Provisioner) Stop(ctx context.Context, name string) error {
	return p.withProvider(ctx, name, "stop", func(ctx context.Context, clusterName string) error {
		return p.infraProvider.StopNodes(ctx, clusterName)
	})
}

// List returns all vCluster clusters by querying the Docker infrastructure provider.
func (p *Provisioner) List(ctx context.Context) ([]string, error) {
	if p.infraProvider == nil {
		return nil, fmt.Errorf("%w for vCluster list", clustererr.ErrProviderNotSet)
	}

	clusters, err := p.infraProvider.ListAllClusters(ctx)
	if err != nil {
		return nil, fmt.Errorf("failed to list vClusters: %w", err)
	}

	return clusters, nil
}

// Exists checks if a vCluster cluster exists by querying the Docker infrastructure provider.
func (p *Provisioner) Exists(ctx context.Context, name string) (bool, error) {
	target := p.resolveName(name)

	clusters, err := p.List(ctx)
	if err != nil {
		return false, fmt.Errorf("failed to list vClusters: %w", err)
	}

	return slices.Contains(clusters, target), nil
}

// --- internals ---

// withProvider executes a provider operation with proper nil check and error wrapping.
func (p *Provisioner) withProvider(
	ctx context.Context,
	name string,
	operationName string,
	providerFunc func(ctx context.Context, clusterName string) error,
) error {
	target := p.resolveName(name)

	err := clustererr.RunProviderOp(
		ctx, p.infraProvider, target, operationName, providerFunc,
	)
	if err != nil {
		return fmt.Errorf("vcluster provider op: %w", err)
	}

	return nil
}

// buildValuesFiles returns the ordered list of Helm values files for CreateDocker.
// A temp file with the default Kubernetes version is prepended so the user's
// values file (if present) can override it. The returned cleanup function removes
// the temp file and must be deferred by the caller.
func buildValuesFiles(userValuesPath string) ([]string, func(), error) {
	defaultsContent := fmt.Sprintf(
		"controlPlane:\n  distro:\n    k8s:\n      image:\n        tag: %s\n",
		defaultKubernetesVersion,
	)

	tmpFile, err := os.CreateTemp("", "ksail-vcluster-defaults-*.yaml")
	if err != nil {
		return nil, func() {}, fmt.Errorf("create temp values file: %w", err)
	}

	tmpName := tmpFile.Name()
	cleanupFn := func() { _ = os.Remove(tmpName) }

	_, writeErr := tmpFile.WriteString(defaultsContent)

	closeErr := tmpFile.Close()

	if writeErr != nil {
		cleanupFn()

		return nil, func() {}, fmt.Errorf("write default values: %w", writeErr)
	}

	if closeErr != nil {
		cleanupFn()

		return nil, func() {}, fmt.Errorf("close default values file: %w", closeErr)
	}

	// Defaults first, user values second â€” later files override earlier ones.
	result := []string{tmpName}
	if strings.TrimSpace(userValuesPath) != "" {
		result = append(result, userValuesPath)
	}

	return result, cleanupFn, nil
}

func (p *Provisioner) resolveName(name string) string {
	if strings.TrimSpace(name) != "" {
		return name
	}

	return p.name
}

// newGlobalFlags creates a minimal GlobalFlags for the vCluster Go SDK.
// Config is set to the default path (~/.vcluster/config.json) so that
// OCI image caches are stored persistently across runs.
func newGlobalFlags() *flags.GlobalFlags {
	configPath, err := cliconfig.DefaultFilePath()
	if err != nil {
		configPath = ""
	}

	return &flags.GlobalFlags{
		Config: configPath,
	}
}

// newStreamLogger creates a loft-sh/log Logger that writes to stdout.
func newStreamLogger() loftlog.Logger {
	return loftlog.NewStreamLogger(os.Stdout, os.Stderr, logrus.InfoLevel)
}
