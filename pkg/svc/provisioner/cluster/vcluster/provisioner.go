package vclusterprovisioner

import (
	"context"
	"errors"
	"fmt"
	"os"
	"os/exec"
	"path/filepath"
	"slices"
	"strings"
	"time"

	"github.com/devantler-tech/ksail/v5/pkg/svc/provider"
	"github.com/devantler-tech/ksail/v5/pkg/svc/provisioner/cluster/clustererr"
	"github.com/devantler-tech/ksail/v5/pkg/svc/provisioner/cluster/kernelmod"
	loftlog "github.com/loft-sh/log"
	"github.com/loft-sh/vcluster/pkg/cli"
	cliconfig "github.com/loft-sh/vcluster/pkg/cli/config"
	"github.com/loft-sh/vcluster/pkg/cli/flags"
	"github.com/sirupsen/logrus"
)

// defaultVClusterName is used when no name is provided.
const defaultVClusterName = "vcluster-default"

// defaultChartVersion is the vCluster Helm chart version used by the Docker driver.
// This MUST match a published ghcr.io/loft-sh/vcluster-pro tag that supports the
// experimental.docker config generated by the SDK (github.com/loft-sh/vcluster).
// Must be available on ghcr.io/loft-sh/vcluster-pro as an OCI artifact.
const defaultChartVersion = "0.32.0-alpha.2"

// defaultKubernetesVersion overrides the SDK's default Kubernetes version.
// The SDK defaults to v1.35.0, but ghcr.io/loft-sh/kubernetes:v1.35.0-full ships
// a corrupt kine binary (9 bytes "Not Found" instead of ELF) on both amd64 and
// arm64. This causes the vCluster service to crash-loop with "exec format error".
// v1.34.0 is the latest version with a working kine binary.
// Remove this override once the upstream v1.35.0 image is fixed.
const defaultKubernetesVersion = "v1.34.0"

// connectMaxAttempts is the number of times to retry ConnectDocker after cluster
// creation. The SDK's waitForVCluster has a hardcoded 3-minute readiness timeout
// which is too short for CI runners. Retrying gives an effective timeout of
// ~9 minutes (3 attempts x 3 minutes).
const connectMaxAttempts = 3

// controlPlaneContainerPrefix is the Docker container name prefix used by the
// vCluster SDK for control plane containers.
const controlPlaneContainerPrefix = "vcluster.cp."

// dbusWaitTimeout is how long to wait for D-Bus to become available inside the
// control plane container. On CI runners (GitHub Actions), systemd inside the
// container may take several seconds to initialize D-Bus after container start.
const dbusWaitTimeout = 30 * time.Second

// dbusWaitInterval is the polling interval when waiting for D-Bus readiness.
const dbusWaitInterval = 500 * time.Millisecond

// dbusErrorSubstring identifies the D-Bus startup race condition error.
// The SDK's install-standalone.sh runs "systemctl restart systemd-journald"
// which fails when D-Bus hasn't initialized yet inside the privileged container.
const dbusErrorSubstring = "Failed to connect to bus"

// errDBusTimeout is returned when D-Bus does not become available
// within the configured timeout.
var errDBusTimeout = errors.New("D-Bus socket did not appear within timeout")

// errEmptyJoinToken is returned when the SDK's persisted join token
// file exists but is empty.
var errEmptyJoinToken = errors.New("join token file is empty")

// Provisioner implements the cluster provisioner interface for vCluster's Docker
// driver (Vind). Create and Delete use the vCluster Go SDK directly, while
// Start/Stop/List/Exists delegate to the Docker infrastructure provider.
type Provisioner struct {
	name          string
	valuesPath    string
	infraProvider provider.Provider
}

// NewProvisioner constructs a new vCluster provisioner.
//
// Parameters:
//   - name: default cluster name (used when no name is passed to methods)
//   - valuesPath: optional path to a vcluster.yaml values file
//   - infraProvider: Docker infrastructure provider for Start/Stop/List/Exists
func NewProvisioner(
	name string,
	valuesPath string,
	infraProvider provider.Provider,
) *Provisioner {
	if name == "" {
		name = defaultVClusterName
	}

	return &Provisioner{
		name:          name,
		valuesPath:    valuesPath,
		infraProvider: infraProvider,
	}
}

// SetProvider sets the infrastructure provider for node operations.
// This implements the ProviderAware interface.
func (p *Provisioner) SetProvider(prov provider.Provider) {
	p.infraProvider = prov
}

// Create provisions a vCluster using the Docker driver via the vCluster Go SDK.
//
// Cluster creation and connect are split into two phases because the SDK's
// ConnectDocker has a hardcoded 3-minute readiness timeout (waitForVCluster)
// which is too short for CI runners. We retry ConnectDocker up to
// connectMaxAttempts times, giving an effective timeout of ~9 minutes.
//
// On CI runners (GitHub Actions), the SDK's install-standalone.sh may fail
// because systemd inside the privileged container hasn't initialized D-Bus
// yet. When this happens the container is already running, so we wait for
// D-Bus readiness and re-run the install script.
//
// On Linux, this method ensures the br_netfilter kernel module is loaded before
// creating the cluster, as it's required for Docker bridge networking features.
func (p *Provisioner) Create(ctx context.Context, name string) error {
	target := p.resolveName(name)

	// Ensure required kernel modules are loaded (Linux only)
	err := kernelmod.EnsureBrNetfilter(ctx, os.Stdout)
	if err != nil {
		return fmt.Errorf("failed to ensure kernel modules: %w", err)
	}

	opts := &cli.CreateOptions{
		Driver:       "docker",
		ChartVersion: defaultChartVersion,
		Connect:      false,
		Upgrade:      false,
		Distro:       "k8s",
	}

	valuesFiles, cleanup, err := buildValuesFiles(p.valuesPath)
	if err != nil {
		return fmt.Errorf("failed to prepare values files: %w", err)
	}
	defer cleanup()

	opts.Values = valuesFiles

	globalFlags := newGlobalFlags()
	logger := newStreamLogger()

	err = cli.CreateDocker(ctx, opts, globalFlags, target, logger)
	if err != nil {
		if !strings.Contains(err.Error(), dbusErrorSubstring) {
			return fmt.Errorf("failed to create vCluster: %w", err)
		}

		logger.Infof("D-Bus not ready in container — recovering...")

		recoverErr := recoverFromDBusError(ctx, globalFlags, target, logger)
		if recoverErr != nil {
			return fmt.Errorf("failed to create vCluster (D-Bus recovery failed): %w", recoverErr)
		}
	}

	return connectWithRetry(ctx, globalFlags, target, logger)
}

// connectWithRetry calls ConnectDocker in a retry loop to work around the SDK's
// hardcoded 3-minute readiness timeout. Each attempt gets a fresh 3-minute
// window; on retry the kubeconfig is already available so only the readiness
// poll is retried.
func connectWithRetry(
	ctx context.Context,
	globalFlags *flags.GlobalFlags,
	clusterName string,
	logger loftlog.Logger,
) error {
	connectOpts := &cli.ConnectOptions{
		UpdateCurrent: true,
	}

	var lastErr error

	for attempt := range connectMaxAttempts {
		if attempt > 0 {
			logger.Infof(
				"Retrying vCluster connect (attempt %d/%d)...",
				attempt+1, connectMaxAttempts,
			)
		}

		lastErr = cli.ConnectDocker(
			ctx, connectOpts, globalFlags, clusterName, nil, logger,
		)
		if lastErr == nil {
			return nil
		}

		logger.Warnf(
			"vCluster connect attempt %d/%d failed: %v",
			attempt+1, connectMaxAttempts, lastErr,
		)
	}

	return fmt.Errorf(
		"failed to connect to vCluster after %d attempts: %w",
		connectMaxAttempts, lastErr,
	)
}

// Delete removes a vCluster using the Docker driver via the vCluster Go SDK.
// Returns clustererr.ErrClusterNotFound if the cluster does not exist.
func (p *Provisioner) Delete(ctx context.Context, name string) error {
	target := p.resolveName(name)

	exists, err := p.Exists(ctx, target)
	if err != nil {
		return fmt.Errorf("failed to check cluster existence: %w", err)
	}

	if !exists {
		return fmt.Errorf("%w: %s", clustererr.ErrClusterNotFound, target)
	}

	opts := &cli.DeleteOptions{
		Driver:         "docker",
		DeleteContext:  true,
		IgnoreNotFound: true,
	}

	globalFlags := newGlobalFlags()
	logger := newStreamLogger()

	// platformClient is nil for local Docker-based clusters (no platform integration).
	err = cli.DeleteDocker(ctx, nil, opts, globalFlags, target, logger)
	if err != nil {
		return fmt.Errorf("failed to delete vCluster: %w", err)
	}

	return nil
}

// Start starts a stopped vCluster by starting its Docker containers.
// Delegates to the infrastructure provider for container operations.
func (p *Provisioner) Start(ctx context.Context, name string) error {
	return p.withProvider(ctx, name, "start", func(ctx context.Context, clusterName string) error {
		return p.infraProvider.StartNodes(ctx, clusterName)
	})
}

// Stop stops a running vCluster by stopping its Docker containers.
// Delegates to the infrastructure provider for container operations.
func (p *Provisioner) Stop(ctx context.Context, name string) error {
	return p.withProvider(ctx, name, "stop", func(ctx context.Context, clusterName string) error {
		return p.infraProvider.StopNodes(ctx, clusterName)
	})
}

// List returns all vCluster clusters by querying the Docker infrastructure provider.
func (p *Provisioner) List(ctx context.Context) ([]string, error) {
	if p.infraProvider == nil {
		return nil, fmt.Errorf("%w for vCluster list", clustererr.ErrProviderNotSet)
	}

	clusters, err := p.infraProvider.ListAllClusters(ctx)
	if err != nil {
		return nil, fmt.Errorf("failed to list vClusters: %w", err)
	}

	return clusters, nil
}

// Exists checks if a vCluster cluster exists by querying the Docker infrastructure provider.
func (p *Provisioner) Exists(ctx context.Context, name string) (bool, error) {
	target := p.resolveName(name)

	clusters, err := p.List(ctx)
	if err != nil {
		return false, fmt.Errorf("failed to list vClusters: %w", err)
	}

	return slices.Contains(clusters, target), nil
}

// --- D-Bus recovery ---

// recoverFromDBusError handles the case where CreateDocker fails because
// systemd inside the container hasn't initialized D-Bus yet. The container
// is already running at this point — we wait for D-Bus and re-run the
// install script that the SDK originally attempted.
func recoverFromDBusError(
	ctx context.Context,
	globalFlags *flags.GlobalFlags,
	clusterName string,
	logger loftlog.Logger,
) error {
	containerName := controlPlaneContainerPrefix + clusterName

	err := waitForDBus(ctx, containerName)
	if err != nil {
		return fmt.Errorf("D-Bus never became available: %w", err)
	}

	logger.Infof("D-Bus ready, re-running install script...")

	token, err := readJoinToken(globalFlags, clusterName)
	if err != nil {
		return fmt.Errorf("failed to read join token: %w", err)
	}

	err = rerunInstallScript(ctx, containerName, clusterName, token)
	if err != nil {
		return fmt.Errorf("install script re-run failed: %w", err)
	}

	return nil
}

// waitForDBus polls the container until the D-Bus system socket exists,
// indicating that systemd has initialized far enough for systemctl to work.
func waitForDBus(ctx context.Context, containerName string) error {
	deadline := time.Now().Add(dbusWaitTimeout)

	for time.Now().Before(deadline) {
		select {
		case <-ctx.Done():
			return fmt.Errorf("context cancelled while waiting for D-Bus: %w", ctx.Err())
		default:
		}

		cmd := exec.CommandContext(ctx, "docker", "exec", containerName,
			"test", "-e", "/run/dbus/system_bus_socket")

		if cmd.Run() == nil {
			return nil
		}

		time.Sleep(dbusWaitInterval)
	}

	return fmt.Errorf("%w: %v", errDBusTimeout, dbusWaitTimeout)
}

// readJoinToken reads the join token persisted by the SDK during container
// creation. The token is stored alongside the SDK's config directory at
// docker/vclusters/<name>/token.txt.
func readJoinToken(globalFlags *flags.GlobalFlags, clusterName string) (string, error) {
	tokenPath := filepath.Join(
		filepath.Dir(globalFlags.Config),
		"docker", "vclusters", clusterName, "token.txt",
	)

	//nolint:gosec // path is constructed from SDK config dir + cluster name.
	data, err := os.ReadFile(tokenPath)
	if err != nil {
		return "", fmt.Errorf("read %s: %w", tokenPath, err)
	}

	token := strings.TrimSpace(string(data))
	if token == "" {
		return "", fmt.Errorf("%w: %s", errEmptyJoinToken, tokenPath)
	}

	return token, nil
}

// rerunInstallScript executes the same install-standalone.sh that the SDK
// originally ran, but now D-Bus is available so systemctl commands succeed.
func rerunInstallScript(
	ctx context.Context,
	containerName string,
	clusterName string,
	joinToken string,
) error {
	scriptURL := fmt.Sprintf(
		"https://github.com/loft-sh/vcluster/releases/download/v%s/install-standalone.sh",
		defaultChartVersion,
	)

	installCmd := fmt.Sprintf(
		"set -e -o pipefail; mount --make-rshared /; "+
			`curl -sfLk "%s" | sh -s -- `+
			"--skip-download --skip-wait "+
			"--vcluster-name %s --join-token %s",
		scriptURL, clusterName, joinToken,
	)

	cmd := exec.CommandContext( //nolint:gosec // args are internally controlled constants.
		ctx, "docker", "exec", containerName, "bash", "-c", installCmd,
	)
	cmd.Stdout = os.Stdout
	cmd.Stderr = os.Stderr

	err := cmd.Run()
	if err != nil {
		return fmt.Errorf("docker exec failed: %w", err)
	}

	return nil
}

// --- internals ---

// withProvider executes a provider operation with proper nil check and error wrapping.
func (p *Provisioner) withProvider(
	ctx context.Context,
	name string,
	operationName string,
	providerFunc func(ctx context.Context, clusterName string) error,
) error {
	target := p.resolveName(name)

	err := clustererr.RunProviderOp(
		ctx, p.infraProvider, target, operationName, providerFunc,
	)
	if err != nil {
		return fmt.Errorf("vcluster provider op: %w", err)
	}

	return nil
}

// buildValuesFiles returns the ordered list of Helm values files for CreateDocker.
// A temp file with the default Kubernetes version is prepended so the user's
// values file (if present) can override it. The returned cleanup function removes
// the temp file and must be deferred by the caller.
func buildValuesFiles(userValuesPath string) ([]string, func(), error) {
	defaultsContent := fmt.Sprintf(
		"controlPlane:\n  distro:\n    k8s:\n      image:\n        tag: %s\n",
		defaultKubernetesVersion,
	)

	tmpFile, err := os.CreateTemp("", "ksail-vcluster-defaults-*.yaml")
	if err != nil {
		return nil, func() {}, fmt.Errorf("create temp values file: %w", err)
	}

	tmpName := tmpFile.Name()
	cleanupFn := func() { _ = os.Remove(tmpName) }

	_, writeErr := tmpFile.WriteString(defaultsContent)

	closeErr := tmpFile.Close()

	if writeErr != nil {
		cleanupFn()

		return nil, func() {}, fmt.Errorf("write default values: %w", writeErr)
	}

	if closeErr != nil {
		cleanupFn()

		return nil, func() {}, fmt.Errorf("close default values file: %w", closeErr)
	}

	// Defaults first, user values second — later files override earlier ones.
	result := []string{tmpName}
	if strings.TrimSpace(userValuesPath) != "" {
		result = append(result, userValuesPath)
	}

	return result, cleanupFn, nil
}

func (p *Provisioner) resolveName(name string) string {
	if strings.TrimSpace(name) != "" {
		return name
	}

	return p.name
}

// newGlobalFlags creates a minimal GlobalFlags for the vCluster Go SDK.
// Config is set to the default path (~/.vcluster/config.json) so that
// OCI image caches are stored persistently across runs.
func newGlobalFlags() *flags.GlobalFlags {
	configPath, err := cliconfig.DefaultFilePath()
	if err != nil {
		configPath = ""
	}

	return &flags.GlobalFlags{
		Config: configPath,
	}
}

// newStreamLogger creates a loft-sh/log Logger that writes to stdout.
func newStreamLogger() loftlog.Logger {
	return loftlog.NewStreamLogger(os.Stdout, os.Stderr, logrus.InfoLevel)
}
